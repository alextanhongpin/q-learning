{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning is a type of reinforcement learning. You have a representation of states `s` and possible actions `a` in those states. The value `Q` is referred to as the *state-action* value (represented as `Q(s, a)`). \n",
    "\n",
    "In Q-learning you start by setting all your state-action to zero (this is not always the case), and you go around and explore state-action space. After you try an action in a state, you evaluate the state that it has lead to. If it leads to a desirable outcome (reward), the weight of the action increase and you will be more likely to choose it again the next time you are in the state. In an undesirable outcome, you will reduce the Q value so that other actions will have greater value and will be choosen instead. When you update Q, you are updating it for the *previous* state-action combination. You can only update Q *after* you have seen the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
