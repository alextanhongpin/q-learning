{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning is a type of reinforcement learning. You have a representation of states `s` and possible actions `a` in those states. The value `Q` is referred to as the *state-action* value (represented as `Q(s, a)`). \n",
    "\n",
    "In Q-learning you start by setting all your state-action to zero (this is not always the case), and you go around and explore state-action space. After you try an action in a state, you evaluate the state that it has lead to. If it leads to a desirable outcome (reward), the weight of the action increase and you will be more likely to choose it again the next time you are in the state. In an undesirable outcome, you will reduce the Q value so that other actions will have greater value and will be choosen instead. When you update Q, you are updating it for the *previous* state-action combination. You can only update Q *after* you have seen the results.\n",
    "\n",
    "The states will be updated using the equation: <br><br>\n",
    "$Q(s, a) += alpha * (reward(s, a) + max(Q(s') - Q(s, a)))$\n",
    "\n",
    "where: <br>\n",
    "$s$ is the previous state <br>\n",
    "$a$ is the previous action <br>\n",
    "$s'$ is the current state <br>\n",
    "$alpha$ is the discount factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the pseudo code for the algorithm:\n",
    "```\n",
    "Initialize Q(s, a) arbitrarily\n",
    "Repeat (for each episode)\n",
    "    Initialize s\n",
    "    Repeat (for each step of episode):\n",
    "        Choose a from s using policy derived from Q\n",
    "            (e.g., E-greedy)\n",
    "        Take action a, observe r, s'\n",
    "        Q(s, a) <-- Q(s, a) + alpha [r + discount * max(Q(s', a') - Q(s, a)]\n",
    "        s <-- s';\n",
    "    until s is terminal\n",
    "```\n",
    "\n",
    "The parameters used in the Q-value update process are:\n",
    "\n",
    "- alpha $\\alpha$ - the learning rate, set between 0 and 1. Setting it to 0 means that the Q-values are never updated, hence nothing is learned. Setting a high value such as 0.9 means that learning can occur quickly.\n",
    "- gamma $\\gamma$ - discount factor, also set between 0 and 1. This models the fact that future reward are worth less than immediate rewards. Mathematically, the discount factor needs to be set that 0 for the algorithm to converge (typically at 0.8).\n",
    "- $max_\\alpha$ - the maximum reward that is attainable in the state following the current one, i.e. the reward for taking the optimal action thereafter.\n",
    "\n",
    "\n",
    "The steps above described in plain english:\n",
    "\n",
    "1. Initialize the Q-values table, $Q(s,a)$\n",
    "2. Observe the current state, $s$\n",
    "3. Choose an action, $a$, for that state based on one of the action selection policies ($\\epsilon$-soft, $\\epsilon$-greedy or softmax). Links [here](http://www.cse.unsw.edu.au/~cs9417ml/RL1/tdlearning.html#aselection).\n",
    "4. Take the action, and observe the reward $r$, as well as the new state, $s'$\n",
    "5. Update the Q-value for the state using the observed reward and the maximum reward possible for the next state. The updating is done according to the formula and parameters described above. \n",
    "6. Set the state to the new state, and repeat the process until a terminal state is reached.\n",
    "\n",
    "Reference [link](http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
